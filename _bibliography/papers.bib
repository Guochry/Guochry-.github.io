---
---

@string{aps = {American Physical Society,}}

@inproceedings{guobeyond,
  title={Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment},
  author={Guo, Geyang* and Zhao, Ranchi* and Tang, Tianyi and Zhao, Xin and Wen, Ji-Rong},
  booktitle={The Twelfth International Conference on Learning Representations},
  preview		 = {FIGA.jpg},
  selected     = {true},
  arxiv = {2311.04072},
  code = {https://github.com/RUCAIBox/FIGA},
  abstract={Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.},
  year={2023}
}

@inproceedings{guo2023towards,
  title={Towards effective ancient chinese translation: Dataset, model, and evaluation},
  author={Guo, Geyang and Yang, Jiarong and Lu, Fengyuan and Qin, Jiaxin and Tang, Tianyi and Zhao, Wayne Xin},
  booktitle={CCF International Conference on Natural Language Processing and Chinese Computing},
  preview		 = {Erya.jpg},
  selected     = {true},
  arxiv = {2308.00240},
  code = {https://github.com/RUCAIBox/Erya},
  abstract={Interpreting ancient Chinese has been the key to comprehending vast Chinese literature, tradition, and civilization. In this paper, we propose Erya for ancient Chinese translation. From a dataset perspective, we collect, clean, and classify ancient Chinese materials from various sources, forming the most extensive ancient Chinese resource to date. From a model perspective, we devise Erya training method oriented towards ancient Chinese. We design two jointly-working tasks: disyllabic aligned substitution (DAS) and dual masked language model (DMLM). From an evaluation perspective, we build a benchmark to judge ancient Chinese translation quality in different scenarios and evaluate the ancient Chinese translation capacities of various existing models. Our model ex- hibits remarkable zero-shot performance across five domains, with over +12.0 BLEU against GPT-3.5 models and better human evaluation re- sults than ERNIE Bot. Subsequent fine-tuning further shows the superior transfer capability of Erya model with +6.2 BLEU gain.},
  year={2023},
  organization={Springer}
}

